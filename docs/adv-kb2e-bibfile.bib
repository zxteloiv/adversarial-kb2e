@article{Dirac1953888,
  title   = "The lorentz transformation and absolute time",
  journal = "Physica ",
  volume  = "19",
  number  = "1-–12",
  pages   = "888--896",
  year    = "1953",
  doi     = "10.1016/S0031-8914(53)80099-6",
  author  = "P.A.M. Dirac"
}

@article{Feynman1963118,
  title   = "The theory of a general quantum system interacting with a linear dissipative system",
  journal = "Annals of Physics ",
  volume  = "24",
  pages   = "118--173",
  year    = "1963",
  doi     = "10.1016/0003-4916(63)90068-X",
  author  = "R.P Feynman and F.L {Vernon Jr.}"
}

@inbook{TransE2013,
  title = {Translating Embeddings for Modeling Multi-relational Data},
  booktitle = {Advances in Neural Information Processing Systems 26},
  publisher = {Curran Associates, Inc.},
  author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
  year = {2013},
  pages = {2787–2795}
}

@inproceedings{TransH2014,
  title={Knowledge Graph Embedding by Translating on Hyperplanes.},
  author={Wang, Zhen and Zhang, Jianwen and Feng, Jianlin and Chen, Zheng},
  booktitle={AAAI},
  pages={1112--1119},
  year={2014}
}


@inproceedings{TransR2015,
  title={Learning Entity and Relation Embeddings for Knowledge Graph Completion.},
  author={Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Liu, Yang and Zhu, Xuan},
  booktitle={AAAI},
  pages={2181--2187},
  year={2015}
}

@inbook{NTN,
  title={Reasoning with neural tensor networks for knowledge base completion},
  booktitle={Advances in Neural Information Processing Systems},
  author={Socher, Richard and Chen, Danqi and Manning, Christopher D. and Ng, Andrew},
  year={2013},
  pages={926–934}
}

@inbook{GAN,
  title={Generative adversarial nets},
  booktitle={Advances in neural information processing systems},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year={2014},
  pages={2672–2680}
}

@article{TransG, title={TransG : A Generative Model for Knowledge Graph Embedding}, volume={1}, DOI={10.18653/v1/P16-1219}, journal={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, author={Xiao, Han and Huang, Minlie and Zhu, Xiaoyan}, year={2016}, pages={2316–2325}}

@inbook{IRGAN, place={New York, NY, USA}, series={SIGIR ’17}, title={IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models}, ISBN={978-1-4503-5022-8}, url={http://doi.acm.org/10.1145/3077136.3080786}, DOI={10.1145/3077136.3080786}, abstractNote={This paper provides a unified account of two schools of thinking in information retrieval modelling: the generative retrieval focusing on predicting relevant documents given a query, and the discriminative retrieval focusing on predicting relevancy given a query-document pair. We propose a game theoretical minimax game to iteratively optimise both models. On one hand, the discriminative model, aiming to mine signals from labelled and unlabelled data, provides guidance to train the generative model towards fitting the underlying relevance distribution over documents given the query. On the other hand, the generative model, acting as an attacker to the current discriminative model, generates difficult examples for the discriminative model in an adversarial way by minimising its discrimination objective. With the competition between these two models, we show that the unified framework takes advantage of both schools of thinking: (i) the generative model learns to fit the relevance distribution over documents via the signals from the discriminative model, and (ii) the discriminative model is able to exploit the unlabelled data selected by the generative model to achieve a better estimation for document ranking. Our experimental results have demonstrated significant performance gains as much as 23.96% on Precision@5 and 15.50% on MAP over strong baselines in a variety of applications including web search, item recommendation, and question answering.}, booktitle={Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval}, publisher={ACM}, author={Wang, Jun and Yu, Lantao and Zhang, Weinan and Gong, Yu and Xu, Yinghui and Wang, Benyou and Zhang, Peng and Zhang, Dell}, year={2017}, pages={515–524}, collection={SIGIR ’17}}

@article{VAE, title={Auto-Encoding Variational Bayes}, url={http://arxiv.org/abs/1312.6114}, abstractNote={How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.}, note={arXiv: 1312.6114}, journal={arXiv:1312.6114 [cs, stat]}, author={Kingma, Diederik P. and Welling, Max}, year={2013}, month={Dec}}

@article{Williams_1992, title={Simple statistical gradient-following algorithms for connectionist reinforcement learning}, volume={8}, number={3–4}, journal={Machine learning}, author={Williams, Ronald J.}, year={1992}, pages={229–256}}

@article{GumbelSoftmax_Jiang_2016, title={Categorical Reparameterization with Gumbel-Softmax}, url={http://arxiv.org/abs/1611.01144}, abstractNote={Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.}, note={arXiv: 1611.01144}, journal={arXiv:1611.01144 [cs, stat]}, author={Jang, Eric and Gu, Shixiang and Poole, Ben}, year={2016}, month={Nov}}




