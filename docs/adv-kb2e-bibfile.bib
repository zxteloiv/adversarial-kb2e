
@incollection{TransE2013,
  title = {Translating Embeddings for Modeling Multi-relational Data},
  booktitle = {Advances in Neural Information Processing Systems (NIPS 26)},
  author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
  year = {2013},
  pages = {2787–2795}
}

@inproceedings{TransH2014,
  title={Knowledge Graph Embedding by Translating on Hyperplanes.},
  author={Wang, Zhen and Zhang, Jianwen and Feng, Jianlin and Chen, Zheng},
  booktitle={AAAI},
  pages={1112--1119},
  year={2014}
}


@inproceedings{TransR2015,
  title={Learning Entity and Relation Embeddings for Knowledge Graph Completion.},
  author={Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Liu, Yang and Zhu, Xuan},
  booktitle={AAAI},
  pages={2181--2187},
  year={2015}
}

@inbook{NTN,
  title={Reasoning with neural tensor networks for knowledge base completion},
  booktitle={Advances in Neural Information Processing Systems},
  author={Socher, Richard and Chen, Danqi and Manning, Christopher D. and Ng, Andrew},
  year={2013},
  pages={926–934}
}

@inbook{GAN,
  title={Generative adversarial nets},
  booktitle={Advances in neural information processing systems},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year={2014},
  pages={2672–2680}
}

@article{TransG, title={TransG : A Generative Model for Knowledge Graph Embedding}, volume={1}, DOI={10.18653/v1/P16-1219}, journal={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, author={Xiao, Han and Huang, Minlie and Zhu, Xiaoyan}, year={2016}, pages={2316–2325}}

@inbook{IRGAN, place={New York, NY, USA}, series={SIGIR ’17}, title={IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models}, ISBN={978-1-4503-5022-8}, url={http://doi.acm.org/10.1145/3077136.3080786}, DOI={10.1145/3077136.3080786}, abstractNote={This paper provides a unified account of two schools of thinking in information retrieval modelling: the generative retrieval focusing on predicting relevant documents given a query, and the discriminative retrieval focusing on predicting relevancy given a query-document pair. We propose a game theoretical minimax game to iteratively optimise both models. On one hand, the discriminative model, aiming to mine signals from labelled and unlabelled data, provides guidance to train the generative model towards fitting the underlying relevance distribution over documents given the query. On the other hand, the generative model, acting as an attacker to the current discriminative model, generates difficult examples for the discriminative model in an adversarial way by minimising its discrimination objective. With the competition between these two models, we show that the unified framework takes advantage of both schools of thinking: (i) the generative model learns to fit the relevance distribution over documents via the signals from the discriminative model, and (ii) the discriminative model is able to exploit the unlabelled data selected by the generative model to achieve a better estimation for document ranking. Our experimental results have demonstrated significant performance gains as much as 23.96% on Precision@5 and 15.50% on MAP over strong baselines in a variety of applications including web search, item recommendation, and question answering.}, booktitle={Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval}, publisher={ACM}, author={Wang, Jun and Yu, Lantao and Zhang, Weinan and Gong, Yu and Xu, Yinghui and Wang, Benyou and Zhang, Peng and Zhang, Dell}, year={2017}, pages={515–524}, collection={SIGIR ’17}}

@article{VAE, title={Auto-Encoding Variational Bayes}, url={http://arxiv.org/abs/1312.6114}, abstractNote={How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.}, note={arXiv: 1312.6114}, journal={arXiv:1312.6114 [cs, stat]}, author={Kingma, Diederik P. and Welling, Max}, year={2013}, month={Dec}}

@article{Williams_1992, title={Simple statistical gradient-following algorithms for connectionist reinforcement learning}, volume={8}, number={3–4}, journal={Machine learning}, author={Williams, Ronald J.}, year={1992}, pages={229–256}}

@article{GumbelSoftmax_Jiang_2016, title={Categorical Reparameterization with Gumbel-Softmax}, url={http://arxiv.org/abs/1611.01144}, abstractNote={Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.}, note={arXiv: 1611.01144}, journal={arXiv:1611.01144 [cs, stat]}, author={Jang, Eric and Gu, Shixiang and Poole, Ben}, year={2016}, month={Nov}}

@article{Salimans_2016, title={Improved Techniques for Training GANs}, url={http://arxiv.org/abs/1606.03498}, abstractNote={We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.}, note={arXiv: 1606.03498}, journal={arXiv:1606.03498 [cs]}, author={Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi}, year={2016}, month={Jun}}

@article{dietterich2000ensemble,
  title={Ensemble methods in machine learning},
  author={Dietterich, Thomas G and others},
  journal={Multiple classifier systems},
  volume={1857},
  pages={1--15},
  year={2000},
  publisher={Springer}
}

@inproceedings{TransD,
  title={Knowledge Graph Embedding via Dynamic Mapping Matrix.},
  author={Ji, Guoliang and He, Shizhu and Xu, Liheng and Liu, Kang and Zhao, Jun},
  booktitle={ACL (1)},
  pages={687--696},
  year={2015}
}

@article{Goodfellow_2016, title={NIPS 2016 Tutorial: Generative Adversarial Networks}, url={http://arxiv.org/abs/1701.00160}, abstractNote={This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.}, note={arXiv: 1701.00160}, journal={arXiv:1701.00160 [cs]}, author={Goodfellow, Ian}, year={2016}, month={Dec}}

@inproceedings{bordes2011structured_embedding,
  title={Learning Structured Embeddings of Knowledge Bases.},
  author={Bordes, Antoine and Weston, Jason and Collobert, Ronan and Bengio, Yoshua and others},
  booktitle={AAAI},
  volume={6},
  number={1},
  pages={6},
  year={2011}
}

@inproceedings{jenatton2012bilinear,
  title={A latent factor model for highly multi-relational data},
  author={Jenatton, Rodolphe and Roux, Nicolas L and Bordes, Antoine and Obozinski, Guillaume R},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3167--3175},
  year={2012}
}

@InProceedings{Ledig_2017_CVPR,
author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
title = {Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
} 

@article{Arjovsky2017WGAN, title={Wasserstein GAN}, url={http://arxiv.org/abs/1701.07875}, abstractNote={We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.}, note={arXiv: 1701.07875}, journal={arXiv:1701.07875 [cs, stat]}, author= "Arjovsky, Martin and Chintala, Soumith and Bottou, Léon", year={2017}, month={Jan}}

@article{Gulrajani2017LipschitzReg, title={Improved Training of Wasserstein GANs}, url={http://arxiv.org/abs/1704.00028}, abstractNote={Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes significant progress toward stable training of GANs, but can still generate low-quality samples or fail to converge in some settings. We find that these training failures are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to pathological behavior. We propose an alternative method for enforcing the Lipschitz constraint: instead of clipping weights, penalize the norm of the gradient of the critic with respect to its input. Our proposed method converges faster and generates higher-quality samples than WGAN with weight clipping. Finally, our method enables very stable GAN training: for the first time, we can train a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data.}, note={arXiv: 1704.00028}, journal={arXiv:1704.00028 [cs, stat]}, author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron}, year={2017}, month={Mar}}

@InProceedings{SeqGAN, title={SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient}, url={https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14344}, abstractNote={As a new way of training generative models, Generative Adversarial Net (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.}, booktitle={Thirty-First AAAI Conference on Artificial Intelligence}, author={Yu, Lantao and Zhang, Weinan and Wang, Jun and Yu, Yong}, year={2017}, month={Feb}}

@article{RankGAN, title={Adversarial Ranking for Language Generation}, url={http://arxiv.org/abs/1705.11001}, abstractNote={Generative adversarial networks (GANs) have great successes on synthesizing data. However, the existing GANs restrict the discriminator to be a binary classifier, and thus limit their learning capacity for tasks that need to synthesize output with rich structures such as natural language descriptions. In this paper, we propose a novel generative adversarial network, RankGAN, for generating high-quality language descriptions. Rather than train the discriminator to learn and assign absolute binary predicate for individual data sample, the proposed RankGAN is able to analyze and rank a collection of human-written and machine-written sentences by giving a reference group. By viewing a set of data samples collectively and evaluating their quality through relative ranking scores, the discriminator is able to make better assessment which in turn helps to learn a better generator. The proposed RankGAN is optimized through the policy gradient technique. Experimental results on multiple public datasets clearly demonstrate the effectiveness of the proposed approach.}, note={arXiv: 1705.11001}, journal={arXiv:1705.11001 [cs]}, author={Lin, Kevin and Li, Dianqi and He, Xiaodong and Zhang, Zhengyou and Sun, Ming-Ting}, year={2017}, month={May}}

@article{EnsembleAdvTraining, title={Ensemble Adversarial Training: Attacks and Defenses}, url={http://arxiv.org/abs/1705.07204}, abstractNote={Machine learning models are vulnerable to adversarial examples, inputs maliciously perturbed to mislead the model. These inputs transfer between models, thus enabling black-box attacks against deployed models. Adversarial training increases robustness to attacks by injecting adversarial examples into training data. Surprisingly, we find that although adversarially trained models exhibit strong robustness to some white-box attacks (i.e., with knowledge of the model parameters), they remain highly vulnerable to transferred adversarial examples crafted on other models. We show that the reason for this vulnerability is the model’s decision surface exhibiting sharp curvature in the vicinity of the data points, thus hindering attacks based on first-order approximations of the model’s loss, but permitting black-box attacks that use adversarial examples transferred from another model. We harness this observation in two ways: First, we propose a simple yet powerful novel attack that first applies a small random perturbation to an input, before finding the optimal perturbation under a first-order approximation. Our attack outperforms prior “single-step” attacks on models trained with or without adversarial training. Second, we propose Ensemble Adversarial Training, an extension of adversarial training that additionally augments training data with perturbed inputs transferred from a number of fixed pre-trained models. On MNIST and ImageNet, ensemble adversarial training vastly improves robustness to black-box attacks.}, note={arXiv: 1705.07204}, journal={arXiv:1705.07204 [cs, stat]}, author={Tramèr, Florian and Kurakin, Alexey and Papernot, Nicolas and Boneh, Dan and McDaniel, Patrick}, year={2017}, month={May}}

@article{Cai2017KBGAN, title={KBGAN: Adversarial Learning for Knowledge Graph Embeddings}, url={http://arxiv.org/abs/1711.04071}, abstractNote={We introduce an adversarial learning framework, which we named KBGAN, to improve the performances of a wide range of existing knowledge graph embedding models. Because knowledge graph datasets typically only contain positive facts, sampling useful negative training examples is a non-trivial task. Replacing the head or tail entity of a fact with a uniformly randomly selected entity is a conventional method for generating negative facts used by many previous works, but the majority of negative facts generated in this way can be easily discriminated from positive facts, and will contribute little towards the training. Inspired by generative adversarial networks (GANs), we use one knowledge graph embedding model as a negative sample generator to assist the training of our desired model, which acts as the discriminator in GANs. The objective of the generator is to generate difficult negative samples that can maximize their likeliness determined by the discriminator, while the discriminator minimizes its training loss. This framework is independent of the concrete form of generator and discriminator, and therefore can utilize a wide variety of knowledge graph embedding models as its building blocks. In experiments, we adversarially train two translation-based models, TransE and TransD, each with assistance from one of the two probability-based models, DistMult and ComplEx. We evaluate the performances of KBGAN on the link prediction task, using three knowledge base completion datasets: FB15k-237, WN18 and WN18RR. Experimental results show that adversarial training substantially improves the performances of target embedding models under various settings.}, note={arXiv: 1711.04071}, journal={arXiv:1711.04071 [cs]}, author={Cai, Liwei and Wang, William Yang}, year={2017}, month={Nov}}

@article{bordes2014SME,
  title={A semantic matching energy function for learning with multi-relational data},
  author={Bordes, Antoine and Glorot, Xavier and Weston, Jason and Bengio, Yoshua},
  journal={Machine Learning},
  volume={94},
  number={2},
  pages={233--259},
  year={2014},
  publisher={Springer}
}

@article{miller1995wordnet,
  title={WordNet: a lexical database for English},
  author={Miller, George A},
  journal={Communications of the ACM},
  volume={38},
  number={11},
  pages={39--41},
  year={1995},
  publisher={ACM}
}

@inproceedings{bollacker2008freebase,
  title={Freebase: a collaboratively created graph database for structuring human knowledge},
  author={Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
  booktitle={Proceedings of the 2008 ACM SIGMOD international conference on Management of data},
  pages={1247--1250},
  year={2008},
  organization={AcM}
}
